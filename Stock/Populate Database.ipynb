{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Connecting to the database file\n",
    "sqlite_file = \"../mydatabase.db\"\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TABLE TwitterStockName(date TEXT, stockName TEXT, fav INTEGER, likes INTEGER, tweet TEXT, categories TEXT, sensitivity TEXT , UNIQUE(date, stockName) ON CONFLICT REPLACE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubun/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#training Classification\n",
    "\n",
    "def trainCategoriesClassifier():\n",
    "    twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),('tfidf',TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n",
    "    #text_clf = Pipeline([('vect', CountVectorizer()),('tfidf',TfidfTransformer()),('clf', MultinomialNB()),])\n",
    "\n",
    "    #Bayes Naive Train Model\n",
    "    text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "    with open('./pickleObj/CatClassifier.pkl', 'wb') as f: \n",
    "        pickle.dump([text_clf], f)\n",
    "    f.close()\n",
    "    \n",
    "def loadCategoriesClassifier():\n",
    "    with open('./pickleObj/CatClassifier.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "        text_clf = pickle.load(f)\n",
    "    f.close()\n",
    "    return text_clf\n",
    "\n",
    "def getIntoDir(name):\n",
    "    path = './%s' % name\n",
    "    os.chdir(path)\n",
    "def getOutDir():\n",
    "    path = '..'\n",
    "    os.chdir(path)\n",
    "             \n",
    "def loadTweetsFile():\n",
    "    getIntoDir('TweetsData')\n",
    "#     print (os.getcwd())\n",
    "    extension = 'csv'\n",
    "    result = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    \n",
    "    getOutDir()\n",
    "    return result\n",
    "\n",
    "def removeSubString(s,sub):\n",
    "    start = s.find(sub)\n",
    "    end = s.find(' ', start)\n",
    "    if (end ==-1):\n",
    "        end = len(s)\n",
    "    tmp = s[:start] + s[end:]\n",
    "    if (tmp.find(sub)!=-1):\n",
    "        tmp = removeSubString(tmp,sub)\n",
    "    return (tmp)\n",
    "\n",
    "def readFile(fname):\n",
    "    getIntoDir('TweetsData')\n",
    "    ans = []\n",
    "    with open(fname) as f:\n",
    "        reader = csv.reader(f,delimiter=';')\n",
    "        for row in reader:\n",
    "            tmp = [row[0],row[1],row[2],row[3],row[4]]\n",
    "            ans.append(tmp)\n",
    "    getOutDir()\n",
    "    return ans\n",
    "\n",
    "def cleanString(s):\n",
    "    tmp = removeSubString(s,\"http\")\n",
    "    tmp = removeSubString(tmp,\"@\")\n",
    "    tmp = removeSubString(tmp,\"#\")\n",
    "    tmp = removeSubString(tmp,\"&amp\")\n",
    "    tmp = re.sub('[^a-zA-Z ]+', ' ', tmp)\n",
    "    tmp = re.sub( '\\s+', ' ', tmp ).strip()\n",
    "    return tmp.lower()\n",
    "\n",
    "def getCategory(result):\n",
    "    text_clf=loadCategoriesClassifier()[0]\n",
    "#     print (text_clf)\n",
    "    ans = []\n",
    "    for f in result:\n",
    "        ans = ans+readFile(f)\n",
    "    data = np.array(ans)\n",
    "\n",
    "    tweetL = data[:,4].tolist()\n",
    "    tweetLClean=[]\n",
    "    for i in tweetL:\n",
    "        tmp = cleanString(i)\n",
    "        tweetLClean.append(tmp)\n",
    "    return data,text_clf.predict(tweetLClean)\n",
    "\n",
    "def getData():\n",
    "    twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "    data,predicted = getCategory(loadTweetsFile())\n",
    "    cat=twenty_train.target_names\n",
    "    tmp=[]\n",
    "    for i in predicted:\n",
    "        tmp.append(cat[i])\n",
    "    predicted = tmp\n",
    "    predictedNP=np.transpose(np.array(predicted))\n",
    "    ans = np.column_stack((data, predictedNP))\n",
    "    return ans\n",
    "    \n",
    "#trainCategoriesClassifier()\n",
    "\n",
    "finalData = getData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertToDatabase(data,c):\n",
    "    #c is connector SQL\n",
    "    table_name = \"Twitter\"\n",
    "    name_f = \"name\"\n",
    "    fav_f =\"fav\"\n",
    "    like_f = \"likes\"\n",
    "    cat_f = \"categories\"\n",
    "    sen_f = \"sensitivity\"\n",
    "    for d in data:\n",
    "        c.execute(\"INSERT INTO {tn} VALUES(?,?,?,?,?,?)\".format(tn=table_name),d)\n",
    "                  \n",
    "# In[10]:\n",
    "insertToDatabase(finalData,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f41c3817f10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "table_name = \"Twitter\"\n",
    "c.execute(\"INSERT INTO {tn} VALUES(?,?,?,?,?,?)\".format(tn=table_name),finalData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
