{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubun/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7fdfb0820490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connecting to the database file\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parentDir = os.path.dirname(cwd)\n",
    "sqlite_file = parentDir+\"/mydatabase.db\"\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "command=\"CREATE TABLE IF NOT EXISTS Twitter(stockName TEXT, date TEXT, fav INTEGER, likes INTEGER, tweet TEXT, categories TEXT, UNIQUE(date, stockName) ON CONFLICT REPLACE);\"\n",
    "c.execute(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TABLE IF NOT EXISTS TwitterStockName(date TEXT, stockName TEXT, fav INTEGER, likes INTEGER, tweet TEXT, categories TEXT, sensitivity TEXT , UNIQUE(date, stockName) ON CONFLICT REPLACE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parentDir = os.path.dirname(cwd)\n",
    "sqlite_file = parentDir+\"/mydatabase.db\"\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "command=\"CREATE TABLE IF NOT EXISTS Twitter(date TEXT, stockName TEXT, fav INTEGER, likes INTEGER, tweet TEXT, categories TEXT, UNIQUE(date, stockName) ON CONFLICT REPLACE);\"\n",
    "c.execute(command)\n",
    "\n",
    "def getTweetPath():\n",
    "    global parentDir\n",
    "    return (parentDir + \"/Twitter\")\n",
    "\n",
    "def savePickleFile(d,varList):\n",
    "    with open(d, 'wb') as f: \n",
    "        pickle.dump(varList, f)\n",
    "    f.close()\n",
    "def loadPickleFile(d):\n",
    "    with open(d,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "        varList = pickle.load(f)\n",
    "    f.close()\n",
    "    return varList \n",
    "\n",
    "def makeDirectory(folder_name1,sub_folder):\n",
    "    directory = os.path.join(folder_name1 , sub_folder)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "def getOutputDir(folderName,fileName):\n",
    "    global parentDir\n",
    "    outputPath = makeDirectory(parentDir, folderName) \n",
    "    d = outputPath + fileName\n",
    "    return d\n",
    "\n",
    "\n",
    "#training Classification\n",
    "\n",
    "def trainCategoriesClassifier():\n",
    "    twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),('tfidf',TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n",
    "    #text_clf = Pipeline([('vect', CountVectorizer()),('tfidf',TfidfTransformer()),('clf', MultinomialNB()),])\n",
    "\n",
    "    #Bayes Naive Train Model\n",
    "    text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "    savePickleFile(getOutputDir('pickleObj','/classificationObj.pkl'),[text_clf])\n",
    "#     with open('./pickleObj/CatClassifier.pkl', 'wb') as f: \n",
    "#         pickle.dump([text_clf], f)\n",
    "#     f.close()\n",
    "    \n",
    "# def loadCategoriesClassifier():\n",
    "#     with open('./pickleObj/CatClassifier.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#         text_clf = pickle.load(f)\n",
    "#     f.close()\n",
    "#     return text_clf\n",
    "\n",
    "def getIntoDir():\n",
    "    os.chdir(getTweetPath())\n",
    "    \n",
    "def getOutDir():\n",
    "    path = '..'\n",
    "    os.chdir(path)\n",
    "             \n",
    "def loadTweetsFile():\n",
    "    getIntoDir()\n",
    "    extension = 'csv'\n",
    "    result = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    getOutDir()\n",
    "    return result\n",
    "\n",
    "def removeSubString(s,sub):\n",
    "    start = s.find(sub)\n",
    "    end = s.find(' ', start)\n",
    "    if (end ==-1):\n",
    "        end = len(s)\n",
    "    tmp = s[:start] + s[end:]\n",
    "    if (tmp.find(sub)!=-1):\n",
    "        tmp = removeSubString(tmp,sub)\n",
    "    return (tmp)\n",
    "\n",
    "def readFile(fname):\n",
    "    getIntoDir()\n",
    "    ans = []\n",
    "    with open(fname) as f:\n",
    "        reader = csv.reader(f,delimiter=';')\n",
    "        for row in reader:\n",
    "            tmp = [row[0],row[1],row[2],row[3],row[4]]\n",
    "            ans.append(tmp)\n",
    "    getOutDir()\n",
    "    return ans\n",
    "\n",
    "def cleanString(s):\n",
    "    tmp = removeSubString(s,\"http\")\n",
    "    tmp = removeSubString(tmp,\"@\")\n",
    "    tmp = removeSubString(tmp,\"#\")\n",
    "    tmp = removeSubString(tmp,\"&amp\")\n",
    "    tmp = re.sub('[^a-zA-Z ]+', ' ', tmp)\n",
    "    tmp = re.sub( '\\s+', ' ', tmp ).strip()\n",
    "    return tmp.lower()\n",
    "\n",
    "def getCategory(result):\n",
    "    text_clf=loadPickleFile(getOutputDir('pickleObj','/classificationObj.pkl'))[0]\n",
    "#     print (text_clf)\n",
    "    ans = []\n",
    "    for f in result:\n",
    "        ans = ans+readFile(f)\n",
    "    data = np.array(ans)\n",
    "    #remove files\n",
    "    for f in result:\n",
    "        if (os.path.isfile(f)):\n",
    "            os.remove(f)\n",
    "    \n",
    "    tweetL = data[:,4].tolist()\n",
    "    tweetLClean=[]\n",
    "    for i in tweetL:\n",
    "        tmp = cleanString(i)\n",
    "        tweetLClean.append(tmp)\n",
    "    return data,text_clf.predict(tweetLClean)\n",
    "\n",
    "def getData():\n",
    "    twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "    data,predicted = getCategory(loadTweetsFile())\n",
    "    cat=twenty_train.target_names\n",
    "    tmp=[]\n",
    "    for i in predicted:\n",
    "        tmp.append(cat[i])\n",
    "    predicted = tmp\n",
    "    predictedNP=np.transpose(np.array(predicted))\n",
    "    return data,predictedNP\n",
    "    \n",
    "if (not (os.path.isfile(getOutputDir('pickleObj','/classificationObj.pkl')))):\n",
    "    trainCategoriesClassifier()\n",
    "\n",
    "finalData,finalPredicted = getData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324434"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertToDatabase(data,predicted,c):\n",
    "    #c is connector SQL\n",
    "    table_name = \"Twitter\"\n",
    "    for i in range(len(data)):\n",
    "        values = (data[i][0],data[i][1],data[i][2],data[i][3],data[i][4],predicted[i])\n",
    "        c.execute(\"INSERT INTO {tn} VALUES(?,?,?,?,?,?)\".format(tn=table_name),values)\n",
    "                  \n",
    "# In[10]:\n",
    "insertToDatabase(finalData,finalPredicted,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
